"""
Stage 2: Link ID åŒ¹é…
åˆå¹¶åŸ 3_a_match_links.py, 3_b_repair_matching.py, 3_b_manual_repair.py

åŠŸèƒ½:
1. åŸºäºåŒ¹é…è¡¨è‡ªåŠ¨å…³è” å¡å£ â†’ Link_ID
2. ç©ºé—´æœ€è¿‘é‚»ä¿®å¤æœªåŒ¹é…å¡å£
3. äººå·¥ä¿®å¤è¡¨åˆå¹¶ï¼ˆå¯é€‰ï¼‰

ç”¨æ³•:
    python 2_link_matching.py                       # è‡ªåŠ¨åŒ¹é… + ç©ºé—´ä¿®å¤
    python 2_link_matching.py --no-repair           # ä»…è‡ªåŠ¨åŒ¹é…
    python 2_link_matching.py --manual FILE.csv     # é¢å¤–åˆå¹¶äººå·¥ä¿®å¤è¡¨
"""

import pandas as pd
import argparse
import sys
from pathlib import Path

# === è·¯å¾„è®¾ç½® ===
current_file = Path(__file__).resolve()
src_dir = current_file.parent.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from utils.path_manager import pm


# === é…ç½®å‚æ•° ===
CONFIG = {
    # è¾“å…¥
    'input_file': 'checkpoint_flow_std_high_quality.csv',
    'match_table': 'äº¤è­¦å¡å£ä¿¡æ¯è¡¨0315-åŒ¹é…è¡¨.xlsx',
    'road_network': 'jinan_road_network.csv',
    # ç©ºé—´åŒ¹é…é˜ˆå€¼(ç±³)
    'max_distance_meters': 50.0,
    # è¾“å‡º
    'output_matched': 'checkpoint_with_links.csv',
    'output_unmatched': 'unmatched_checkpoints.csv',
    'output_repair_success': 'report_repair_success.csv',
    'output_repair_failed': 'report_repair_failed.csv',
    'output_final': 'checkpoint_with_links_final.csv',
}


def normalize_id(series):
    """æ ‡å‡†åŒ–IDå­—æ®µ"""
    return (series.astype(str)
            .str.replace(r'\.0$', '', regex=True)
            .str.strip())


def step1_auto_match():
    """
    Step 1: åŸºäºåŒ¹é…è¡¨è‡ªåŠ¨åŒ¹é…
    è¿”å›: (matched_df, unmatched_df)
    """
    print("=" * 50)
    print("Step 1: è‡ªåŠ¨åŒ¹é… Link ID")
    print("=" * 50)

    # è¯»å–æµé‡æ•°æ®
    flow_path = pm.get_processed_path(CONFIG['input_file'])
    print(f"è¯»å–æµé‡æ•°æ®: {flow_path.name}")
    flow_df = pd.read_csv(flow_path)

    # è¯»å–åŒ¹é…è¡¨
    match_path = pm.get_raw_path(CONFIG['match_table'])
    print(f"è¯»å–åŒ¹é…è¡¨: {CONFIG['match_table']}")
    match_df = pd.read_excel(match_path)

    # ç»Ÿä¸€IDæ ¼å¼
    flow_df['å¡å£ç¼–å·'] = normalize_id(flow_df['å¡å£ç¼–å·'])
    match_df['å¡å£ç¼–å·'] = normalize_id(match_df['å¡å£ç¼–å·'])

    # æ‰§è¡ŒåŒ¹é… (Left Join)
    cols_to_use = ['å¡å£ç¼–å·', 'Link_ID', 'lon_84', 'lat_84']
    missing_cols = [c for c in cols_to_use if c not in match_df.columns]
    if missing_cols:
        raise KeyError(f"åŒ¹é…è¡¨ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")

    print("æ­£åœ¨æ‰§è¡ŒåŒ¹é…...")
    merged_df = pd.merge(flow_df, match_df[cols_to_use], on='å¡å£ç¼–å·', how='left')

    # åˆ†ç¦»æˆåŠŸ/å¤±è´¥
    success_mask = merged_df['Link_ID'].notna()
    matched_df = merged_df[success_mask].copy()
    unmatched_df = merged_df[~success_mask].copy()

    # æ ¼å¼åŒ– Link_ID
    matched_df['Link_ID'] = matched_df['Link_ID'].astype(int).astype(str)

    # ç»Ÿè®¡
    matched_count = matched_df['å¡å£ç¼–å·'].nunique()
    unmatched_count = unmatched_df['å¡å£ç¼–å·'].nunique()

    print("-" * 40)
    print(f"åŸå§‹å¡å£æ•°: {flow_df['å¡å£ç¼–å·'].nunique()}")
    print(f"åŒ¹é…æˆåŠŸ: {matched_count}")
    print(f"åŒ¹é…å¤±è´¥: {unmatched_count}")
    print("-" * 40)

    # ä¿å­˜æˆåŠŸæ•°æ®
    output_success = pm.get_processed_path(CONFIG['output_matched'])
    matched_df.to_csv(output_success, index=False, encoding='utf-8-sig')
    print(f"âœ… åŒ¹é…æˆåŠŸ: {output_success.name}")

    # ä¿å­˜å¤±è´¥æ•°æ®
    if not unmatched_df.empty:
        unmatched_unique = unmatched_df[['å¡å£ç¼–å·', 'å¡å£åç§°']].drop_duplicates()
        for col in ['lon_84', 'lat_84']:
            if col in unmatched_df.columns:
                first_vals = unmatched_df.groupby('å¡å£ç¼–å·')[col].first()
                unmatched_unique = unmatched_unique.merge(
                    first_vals.reset_index(), on='å¡å£ç¼–å·', how='left'
                )

        output_fail = pm.get_processed_path(CONFIG['output_unmatched'])
        unmatched_unique.to_csv(output_fail, index=False, encoding='utf-8-sig')
        print(f"âš ï¸ æœªåŒ¹é…åå•: {output_fail.name}")
    else:
        print("ğŸ‰ æ‰€æœ‰å¡å£éƒ½åŒ¹é…æˆåŠŸï¼")

    return matched_df, unmatched_df


def step2_spatial_repair(unmatched_df):
    """
    Step 2: ç©ºé—´æœ€è¿‘é‚»ä¿®å¤
    """
    print("\n" + "=" * 50)
    print("Step 2: ç©ºé—´æœ€è¿‘é‚»ä¿®å¤")
    print("=" * 50)

    try:
        import geopandas as gpd
        from shapely import wkt
    except ImportError:
        print("âš ï¸ ç¼ºå°‘ geopandas/shapelyï¼Œè·³è¿‡ç©ºé—´ä¿®å¤")
        print("   å®‰è£…: pip install geopandas shapely")
        return None

    # è¯»å–è·¯ç½‘
    road_path = pm.get_raw_path(CONFIG['road_network'])
    print(f"è¯»å–è·¯ç½‘: {CONFIG['road_network']}")
    road_df = pd.read_csv(road_path)

    # å‡ ä½•è½¬æ¢
    print("å¤„ç†å‡ ä½•æŠ•å½± (WGS84 â†’ UTM Zone 50N)...")
    road_df['geometry'] = road_df['geometry'].apply(wkt.loads)
    gdf_roads = gpd.GeoDataFrame(road_df, geometry='geometry', crs="EPSG:4326")

    # æå–æœªåŒ¹é…å¡å£å”¯ä¸€åæ ‡
    unmatched_unique = unmatched_df[['å¡å£ç¼–å·', 'å¡å£åç§°', 'lon_84', 'lat_84']].drop_duplicates()

    if 'lon_84' not in unmatched_unique.columns or unmatched_unique['lon_84'].isna().all():
        print("âš ï¸ æœªåŒ¹é…å¡å£ç¼ºå°‘æœ‰æ•ˆç»çº¬åº¦ï¼Œè·³è¿‡ç©ºé—´ä¿®å¤")
        return None

    gdf_points = gpd.GeoDataFrame(
        unmatched_unique,
        geometry=gpd.points_from_xy(unmatched_unique.lon_84, unmatched_unique.lat_84),
        crs="EPSG:4326"
    )

    # æŠ•å½±åˆ°ç±³
    gdf_roads_meter = gdf_roads.to_crs("EPSG:32650")
    gdf_points_meter = gdf_points.to_crs("EPSG:32650")

    # ç©ºé—´åŒ¹é…
    max_dist = CONFIG['max_distance_meters']
    print(f"è®¡ç®—æœ€è¿‘é‚» (é˜ˆå€¼: {max_dist}m)...")

    matched_repair = gpd.sjoin_nearest(
        gdf_points_meter,
        gdf_roads_meter[['id', 'geometry']],
        how='left',
        distance_col='dist_meters'
    )

    # åˆ’åˆ†æˆåŠŸ/å¤±è´¥
    success_mask = matched_repair['dist_meters'] <= max_dist
    success_repair = matched_repair[success_mask].copy()
    failed_repair = matched_repair[~success_mask].copy()

    print("-" * 40)
    print(f"å°è¯•ä¿®å¤æ•°: {len(gdf_points)}")
    print(f"æˆåŠŸ (â‰¤{max_dist}m): {len(success_repair)}")
    print(f"å¤±è´¥ (>{max_dist}m): {len(failed_repair)}")
    print("-" * 40)

    # ä¿å­˜æŠ¥å‘Š
    if not success_repair.empty:
        success_repair.rename(columns={'id': 'Matched_LinkID'}, inplace=True)
        report_path = pm.get_processed_path(CONFIG['output_repair_success'])
        success_repair[['å¡å£ç¼–å·', 'å¡å£åç§°', 'Matched_LinkID', 'dist_meters']].to_csv(
            report_path, index=False, encoding='utf-8-sig'
        )
        print(f"âœ… ä¿®å¤æˆåŠŸæŠ¥å‘Š: {report_path.name}")

    if not failed_repair.empty:
        failed_repair.rename(columns={'id': 'Nearest_LinkID'}, inplace=True)
        report_path = pm.get_processed_path(CONFIG['output_repair_failed'])
        failed_repair[['å¡å£ç¼–å·', 'å¡å£åç§°', 'Nearest_LinkID', 'dist_meters']].to_csv(
            report_path, index=False, encoding='utf-8-sig'
        )
        print(f"âš ï¸ ä¿®å¤å¤±è´¥æŠ¥å‘Š: {report_path.name}")

    if success_repair.empty:
        return None

    # å›ææµé‡æ•°æ®
    repair_map = dict(zip(success_repair['å¡å£ç¼–å·'], success_repair['Matched_LinkID']))

    raw_path = pm.get_processed_path(CONFIG['input_file'])
    raw_df = pd.read_csv(raw_path)
    raw_df['å¡å£ç¼–å·'] = normalize_id(raw_df['å¡å£ç¼–å·'])

    repaired_rows = raw_df[raw_df['å¡å£ç¼–å·'].isin(repair_map.keys())].copy()
    repaired_rows['Link_ID'] = repaired_rows['å¡å£ç¼–å·'].map(repair_map)
    repaired_rows['Link_ID'] = repaired_rows['Link_ID'].astype(int).astype(str)

    print(f"å›ææµé‡è®°å½•: {len(repaired_rows)} æ¡")

    return repaired_rows


def step3_manual_repair(main_df, repair_file):
    """
    Step 3: åˆå¹¶äººå·¥ä¿®å¤è¡¨
    """
    print("\n" + "=" * 50)
    print("Step 3: åˆå¹¶äººå·¥ä¿®å¤è¡¨")
    print("=" * 50)

    # å°è¯•å¤šä¸ªè·¯å¾„
    repair_path = pm.get_processed_path(repair_file)
    if not repair_path.exists():
        repair_path = pm.get_raw_path(repair_file)

    if not repair_path.exists():
        print(f"âš ï¸ äººå·¥ä¿®å¤æ–‡ä»¶ä¸å­˜åœ¨: {repair_file}ï¼Œè·³è¿‡")
        return main_df

    print(f"è¯»å–äººå·¥ä¿®å¤è¡¨: {repair_path.name}")
    repair_df = pd.read_csv(repair_path)

    # æ ‡å‡†åŒ–
    repair_df['å¡å£ç¼–å·'] = normalize_id(repair_df['å¡å£ç¼–å·'])
    if 'LinkID' in repair_df.columns:
        repair_df.rename(columns={'LinkID': 'Link_ID'}, inplace=True)
    repair_df['Link_ID'] = normalize_id(repair_df['Link_ID'])

    # æ„å»ºæ˜ å°„
    repair_valid = repair_df[repair_df['Link_ID'].notna() & (repair_df['Link_ID'] != '')]
    repair_map = dict(zip(repair_valid['å¡å£ç¼–å·'], repair_valid['Link_ID']))

    print(f"æœ‰æ•ˆä¿®å¤å¡å£æ•°: {len(repair_map)}")

    # å›ææµé‡
    raw_path = pm.get_processed_path(CONFIG['input_file'])
    raw_df = pd.read_csv(raw_path)
    raw_df['å¡å£ç¼–å·'] = normalize_id(raw_df['å¡å£ç¼–å·'])

    repaired_rows = raw_df[raw_df['å¡å£ç¼–å·'].isin(repair_map.keys())].copy()
    repaired_rows['Link_ID'] = repaired_rows['å¡å£ç¼–å·'].map(repair_map)

    # å»é‡åˆå¹¶
    main_df['å¡å£ç¼–å·'] = normalize_id(main_df['å¡å£ç¼–å·'])
    main_df_safe = main_df[~main_df['å¡å£ç¼–å·'].isin(repair_map.keys())]

    final_df = pd.concat([main_df_safe, repaired_rows], ignore_index=True)

    print("-" * 40)
    print(f"ä¸»è¡¨å¡å£æ•°: {main_df['å¡å£ç¼–å·'].nunique()}")
    print(f"äººå·¥ä¿®å¤æ•°: {len(repair_map)}")
    print(f"åˆå¹¶åå¡å£æ•°: {final_df['å¡å£ç¼–å·'].nunique()}")
    print("-" * 40)

    return final_df


def run(auto_repair=True, manual_repair_file=None):
    """æ‰§è¡Œå®Œæ•´æµç¨‹"""
    # Step 1: è‡ªåŠ¨åŒ¹é…
    matched_df, unmatched_df = step1_auto_match()

    # Step 2: ç©ºé—´ä¿®å¤
    if auto_repair and not unmatched_df.empty:
        repaired_df = step2_spatial_repair(unmatched_df)
        if repaired_df is not None and not repaired_df.empty:
            matched_df = pd.concat([matched_df, repaired_df], ignore_index=True)

    # Step 3: äººå·¥ä¿®å¤
    if manual_repair_file:
        matched_df = step3_manual_repair(matched_df, manual_repair_file)

    # ä¿å­˜æœ€ç»ˆç»“æœ
    output_path = pm.get_processed_path(CONFIG['output_final'])
    matched_df.to_csv(output_path, index=False, encoding='utf-8-sig')

    print("\n" + "=" * 50)
    print("Stage 2 å®Œæˆ")
    print("=" * 50)
    print(f"æœ€ç»ˆå¡å£æ•°: {matched_df['å¡å£ç¼–å·'].nunique()}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path.name}")

    return matched_df


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Stage 2: Link ID åŒ¹é…")
    parser.add_argument('--no-repair', action='store_true',
                        help='ä¸æ‰§è¡Œç©ºé—´ä¿®å¤')
    parser.add_argument('--manual', type=str, default=None,
                        help='äººå·¥ä¿®å¤æ–‡ä»¶å')
    args = parser.parse_args()

    run(auto_repair=not args.no_repair, manual_repair_file=args.manual)
