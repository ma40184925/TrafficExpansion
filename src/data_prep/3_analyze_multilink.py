"""
Stage 3: å¤šå¯¹ä¸€åˆ†æä¸æ¨¡æ¿ç”Ÿæˆ
åŸºäº checkpoint_with_links_final.csv åˆ†æ Link-å¡å£ æ˜ å°„å…³ç³»

åŠŸèƒ½:
1. åˆ†æ Link-å¡å£ æ˜ å°„å…³ç³»
2. ç”Ÿæˆäººå·¥ä¿®æ­£æ¨¡æ¿ï¼ˆè‡ªåŠ¨æ¨æ–­ SUM/MEANï¼‰

ç”¨æ³•:
    python 3_analyze_multilink.py               # åˆ†æå¹¶ç”Ÿæˆæ¨¡æ¿
    python 3_analyze_multilink.py --analyze     # ä»…åˆ†æï¼Œä¸ç”Ÿæˆæ¨¡æ¿
"""

import pandas as pd
import argparse
import sys
from pathlib import Path

# === è·¯å¾„è®¾ç½® ===
current_file = Path(__file__).resolve()
src_dir = current_file.parent.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from utils.path_manager import pm


# === é…ç½®å‚æ•° ===
CONFIG = {
    # è¾“å…¥ (Stage 2 çš„è¾“å‡º)
    'input_file': 'checkpoint_with_links_final.csv',
    # è¾“å‡º
    'output_mapping_report': 'report_link_checkpoint_mapping.csv',
    'output_fix_template': 'manual_fix_config.csv',
}


def normalize_id(series):
    """æ ‡å‡†åŒ–IDå­—æ®µ"""
    return (series.astype(str)
            .str.replace(r'\.0$', '', regex=True)
            .str.strip())


def detect_direction(name):
    """ä»å¡å£åç§°æ£€æµ‹æ–¹å‘"""
    if 'ä¸œå‘è¥¿' in name:
        return 'EW'
    elif 'è¥¿å‘ä¸œ' in name:
        return 'WE'
    elif 'å—å‘åŒ—' in name:
        return 'SN'
    elif 'åŒ—å‘å—' in name:
        return 'NS'
    return 'UNKNOWN'


def infer_aggregation_action(names):
    """æ ¹æ®å¡å£åç§°æ¨æ–­èšåˆæ“ä½œ"""
    directions = [detect_direction(n) for n in names]
    unique_dirs = set(directions)

    # è§„åˆ™1: åŒå‘ -> æ±‚å’Œ
    if ('NS' in unique_dirs and 'SN' in unique_dirs) or \
       ('EW' in unique_dirs and 'WE' in unique_dirs):
        return "SUM", "æ£€æµ‹åˆ°åŒå‘åç§°ï¼Œå»ºè®®æ±‚å’Œ"

    # è§„åˆ™2: æ–¹å‘ç›¸åŒ -> å–å¹³å‡
    if len(unique_dirs) == 1 and list(unique_dirs)[0] != 'UNKNOWN':
        return "MEAN", "æ–¹å‘ç›¸åŒï¼Œå»ºè®®å–å¹³å‡"

    # è§„åˆ™3: åç§°ç›¸ä¼¼ -> å–å¹³å‡
    clean_names = [n.replace('å¡å£', '').replace('(ç¤¼è®©)', '').replace('ï¼ˆç¤¼è®©ï¼‰', '')
                   for n in names]
    if len(set(clean_names)) < len(names):
        return "MEAN", "åç§°é«˜åº¦ç›¸ä¼¼ï¼Œç–‘ä¼¼é‡å¤"

    return "CHECK", "æ— æ³•è‡ªåŠ¨åˆ¤æ–­ï¼Œè¯·äººå·¥æ ¸å®"


def analyze_mapping(df):
    """
    åˆ†æ Link-å¡å£ æ˜ å°„å…³ç³»
    """
    print("=" * 50)
    print("åˆ†æ Link-å¡å£ æ˜ å°„å…³ç³»")
    print("=" * 50)

    # æå–å”¯ä¸€æ˜ å°„ (åªéœ€è¦è¿™ä¸‰åˆ—)
    unique_mapping = df[['Link_ID', 'å¡å£ç¼–å·', 'å¡å£åç§°']].drop_duplicates()

    # èšåˆç»Ÿè®¡
    link_stats = unique_mapping.groupby('Link_ID').agg({
        'å¡å£ç¼–å·': ['count', list],
        'å¡å£åç§°': list
    }).reset_index()

    link_stats.columns = ['Link_ID', 'å¡å£æ•°é‡', 'å¡å£IDåˆ—è¡¨', 'å¡å£åç§°åˆ—è¡¨']

    # åˆ†å¸ƒç»Ÿè®¡
    dist_stats = link_stats['å¡å£æ•°é‡'].value_counts().sort_index()

    print(f"æ€»è·¯æ®µæ•°: {len(link_stats)}")
    print(f"æ€»å¡å£æ•°: {link_stats['å¡å£æ•°é‡'].sum()}")
    print("\næ˜ å°„å…³ç³»åˆ†å¸ƒ:")
    for count, num_links in dist_stats.items():
        print(f"  - {count}ä¸ªå¡å£ â†’ {num_links} æ¡è·¯æ®µ")

    # å¤šå¯¹ä¸€
    multi_links = link_stats[link_stats['å¡å£æ•°é‡'] > 1].copy()

    if not multi_links.empty:
        print(f"\nğŸ” å‘ç° {len(multi_links)} æ¡è·¯æ®µæŒ‚è½½äº†å¤šä¸ªå¡å£")
        print("\n[ç¤ºä¾‹]")
        for _, row in multi_links.head(5).iterrows():
            names = ", ".join(row['å¡å£åç§°åˆ—è¡¨'][:2])
            if len(row['å¡å£åç§°åˆ—è¡¨']) > 2:
                names += f"... (+{len(row['å¡å£åç§°åˆ—è¡¨'])-2})"
            print(f"  Link {row['Link_ID']}: {names}")
    else:
        print("\nğŸ‰ ä¸å­˜åœ¨å¤šå¯¹ä¸€æƒ…å†µ")

    # ä¿å­˜æ˜ å°„æŠ¥å‘Š
    report_df = link_stats.copy()
    report_df['å¡å£åç§°åˆ—è¡¨'] = report_df['å¡å£åç§°åˆ—è¡¨'].apply(lambda x: " | ".join(x))
    report_df['å¡å£IDåˆ—è¡¨'] = report_df['å¡å£IDåˆ—è¡¨'].apply(lambda x: " | ".join(x))

    report_path = pm.get_processed_path(CONFIG['output_mapping_report'])
    report_df.sort_values('å¡å£æ•°é‡', ascending=False).to_csv(
        report_path, index=False, encoding='utf-8-sig'
    )
    print(f"\nğŸ“„ æ˜ å°„æŠ¥å‘Š: {report_path.name}")

    return link_stats, multi_links


def generate_template(multi_links):
    """
    ç”Ÿæˆäººå·¥ä¿®æ­£æ¨¡æ¿
    """
    print("\n" + "=" * 50)
    print("ç”Ÿæˆäººå·¥ä¿®æ­£æ¨¡æ¿")
    print("=" * 50)

    if multi_links.empty:
        print("æ— å¤šå¯¹ä¸€æƒ…å†µï¼Œæ— éœ€ç”Ÿæˆæ¨¡æ¿")
        return

    recommendations = []

    for _, row in multi_links.iterrows():
        link_id = row['Link_ID']
        names = row['å¡å£åç§°åˆ—è¡¨']
        ckpt_ids = row['å¡å£IDåˆ—è¡¨']

        action, reason = infer_aggregation_action(names)

        recommendations.append({
            'Link_ID': link_id,
            'å»ºè®®æ“ä½œ': action,
            'æ¨æ–­ç†ç”±': reason,
            'å¡å£æ•°é‡': len(names),
            'å¡å£åç§°åˆ—è¡¨': " | ".join(names),
            'å¡å£IDåˆ—è¡¨': " | ".join(ckpt_ids)
        })

    rec_df = pd.DataFrame(recommendations)

    # ç»Ÿè®¡
    action_counts = rec_df['å»ºè®®æ“ä½œ'].value_counts()
    print("æ¨æ–­ç»“æœç»Ÿè®¡:")
    for action, count in action_counts.items():
        print(f"  - {action}: {count} æ¡")

    # ä¿å­˜
    output_path = pm.get_processed_path(CONFIG['output_fix_template'])
    rec_df.to_csv(output_path, index=False, encoding='utf-8-sig')

    print(f"\nâœ… æ¨¡æ¿å·²ç”Ÿæˆ: {output_path.name}")
    print("\nã€æ“ä½œæŒ‡å—ã€‘")
    print("1. ç”¨ Excel æ‰“å¼€è¯¥æ–‡ä»¶")
    print("2. æ£€æŸ¥ 'å»ºè®®æ“ä½œ' åˆ—:")
    print("   - SUM: åŒå‘è½¦æµåˆå¹¶ï¼ˆæ±‚å’Œï¼‰")
    print("   - MEAN: é‡å¤æ•°æ®ï¼ˆå–å¹³å‡ï¼‰")
    print("   - CHECK: éœ€äººå·¥æ ¸å®ï¼Œè¯·æ”¹ä¸º SUM æˆ– MEAN")
    print("3. ä¿®æ”¹åä¿å­˜")
    print("4. åç»­è„šæœ¬ä¼šè¯»å–æ­¤æ–‡ä»¶è¿›è¡Œèšåˆå¤„ç†")


def run(analyze_only=False):
    """æ‰§è¡Œæµç¨‹"""
    # è¯»å–æ•°æ®
    input_path = pm.get_processed_path(CONFIG['input_file'])
    print(f"è¯»å–æ•°æ®: {input_path.name}")
    df = pd.read_csv(input_path)
    
    df['Link_ID'] = normalize_id(df['Link_ID'])
    df['å¡å£ç¼–å·'] = normalize_id(df['å¡å£ç¼–å·'])

    # åˆ†ææ˜ å°„
    link_stats, multi_links = analyze_mapping(df)

    # ç”Ÿæˆæ¨¡æ¿
    if not analyze_only and not multi_links.empty:
        generate_template(multi_links)

    return link_stats, multi_links


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Stage 3: å¤šå¯¹ä¸€åˆ†æä¸æ¨¡æ¿ç”Ÿæˆ")
    parser.add_argument('--analyze', action='store_true',
                        help='ä»…åˆ†ææ˜ å°„å…³ç³»ï¼Œä¸ç”Ÿæˆæ¨¡æ¿')
    args = parser.parse_args()

    run(analyze_only=args.analyze)
