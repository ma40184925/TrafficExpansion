"""
Stage 5: åˆå¹¶è·¯ç½‘å±æ€§
å°†è·¯ç½‘çš„é“è·¯ç±»å‹ã€å®½åº¦ã€é•¿åº¦ç­‰å±æ€§åˆå¹¶åˆ°å¡å£æ•°æ®

ç‰¹æ®Šå¤„ç†:
- Link_ID å¯èƒ½æ˜¯å•ä¸ªå€¼ (å¦‚ "17563396") æˆ–å¤šä¸ªå€¼ (å¦‚ "100986499+60322527")
- å¯¹äºå¤šLinkçš„æƒ…å†µï¼Œéœ€è¦åŒ¹é…æ‰€æœ‰Linkå¹¶ä¿ç•™å®Œæ•´ä¿¡æ¯

ç”¨æ³•:
    python 5_merge_road_attrs.py
"""

import pandas as pd
import numpy as np
import argparse
import sys
from pathlib import Path

# === è·¯å¾„è®¾ç½® ===
current_file = Path(__file__).resolve()
src_dir = current_file.parent.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from utils.path_manager import pm


# === é…ç½®å‚æ•° ===
CONFIG = {
    # è¾“å…¥
    'input_checkpoint': 'checkpoint_merged.csv',     # Stage 4 è¾“å‡º
    'input_road_network': 'jinan_road_network.csv',  # è·¯ç½‘æ–‡ä»¶
    # é“è·¯ç±»å‹æ˜ å°„
    'road_kind_mapping': {
        '00': 'é«˜é€Ÿå…¬è·¯',
        '01': 'åŸå¸‚é«˜é€Ÿ',
        '02': 'å›½é“',
        '03': 'çœé“',
        '04': 'å¿é“',
        '06': 'å¸‚é•‡æ‘é“'
    },
    # è¾“å‡º
    'output_with_attrs': 'checkpoint_with_road_attrs.csv',
    'output_road_stats': 'report_road_distribution.csv',
    'output_dropped': 'report_dropped_checkpoints.csv',
}


def normalize_id(series):
    """æ ‡å‡†åŒ–IDå­—æ®µ"""
    return (series.astype(str)
            .str.replace(r'\.0$', '', regex=True)
            .str.strip())


def parse_link_ids(link_id_str):
    """
    è§£æ Link_ID å­—ç¬¦ä¸²ï¼Œå¯èƒ½æ˜¯å•ä¸ªæˆ–å¤šä¸ªï¼ˆç”¨+åˆ†éš”ï¼‰
    è¿”å›: list of link_ids
    """
    link_id_str = str(link_id_str).strip()
    if '+' in link_id_str:
        return [lid.strip() for lid in link_id_str.split('+')]
    else:
        return [link_id_str]


def merge_road_attrs():
    """
    åˆå¹¶è·¯ç½‘å±æ€§åˆ°å¡å£æ•°æ®
    """
    print("=" * 50)
    print("Stage 5: åˆå¹¶è·¯ç½‘å±æ€§")
    print("=" * 50)
    
    # è¯»å–å¡å£æ•°æ®
    ckpt_path = pm.get_processed_path(CONFIG['input_checkpoint'])
    print(f"è¯»å–å¡å£æ•°æ®: {ckpt_path.name}")
    ckpt_df = pd.read_csv(ckpt_path)
    
    # è¯»å–è·¯ç½‘æ•°æ®
    road_path = pm.get_raw_path(CONFIG['input_road_network'])
    print(f"è¯»å–è·¯ç½‘æ•°æ®: {CONFIG['input_road_network']}")
    road_df = pd.read_csv(road_path)
    
    # æ ‡å‡†åŒ–è·¯ç½‘ID
    road_df['id'] = normalize_id(road_df['id'])
    
    # æ ¼å¼åŒ– kind_x (ç¡®ä¿æ˜¯ä¸¤ä½å­—ç¬¦ä¸²ï¼Œå¦‚ '06')
    if 'kind_x' in road_df.columns:
        road_df['kind_x'] = road_df['kind_x'].astype(str).str.zfill(2)
    
    # æ„å»ºè·¯ç½‘å±æ€§å­—å…¸ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾
    # åªä¿ç•™éœ€è¦çš„åˆ—
    road_cols = ['id', 'kind_x', 'width', 'length']
    road_cols = [c for c in road_cols if c in road_df.columns]
    road_dict = road_df.set_index('id')[road_cols[1:]].to_dict('index')
    
    print(f"\nè·¯ç½‘è·¯æ®µæ•°: {len(road_dict)}")
    print(f"å¡å£è®°å½•æ•°: {len(ckpt_df)}")
    print(f"å¡å£æ•°: {ckpt_df['å¡å£ç¼–å·'].nunique()}")
    
    # å¤„ç†æ¯æ¡è®°å½•
    print("\næ­£åœ¨åŒ¹é…è·¯ç½‘å±æ€§...")
    
    results = []
    dropped_records = []
    
    for idx, row in ckpt_df.iterrows():
        link_id_raw = str(row['Link_ID'])
        link_ids = parse_link_ids(link_id_raw)
        
        # æŸ¥æ‰¾æ¯ä¸ª Link çš„å±æ€§
        attrs_list = []
        missing_links = []
        
        for lid in link_ids:
            if lid in road_dict:
                attrs_list.append({
                    'link_id': lid,
                    **road_dict[lid]
                })
            else:
                missing_links.append(lid)
        
        # å¦‚æœæ‰€æœ‰Linkéƒ½æ‰¾ä¸åˆ°ï¼Œè®°å½•ä¸ºdropped
        if not attrs_list:
            dropped_records.append({
                'å¡å£ç¼–å·': row['å¡å£ç¼–å·'],
                'å¡å£åç§°': row.get('å¡å£åç§°', ''),
                'Link_ID': link_id_raw,
                'åŸå› ': f"æ‰€æœ‰Linkå‡æœªåœ¨è·¯ç½‘ä¸­æ‰¾åˆ°: {missing_links}"
            })
            continue
        
        # å¦‚æœéƒ¨åˆ†Linkæ‰¾ä¸åˆ°ï¼Œä»…è­¦å‘Šä½†ç»§ç»­å¤„ç†
        if missing_links:
            # å¯ä»¥é€‰æ‹©è®°å½•è­¦å‘Šï¼Œè¿™é‡Œå…ˆå¿½ç•¥
            pass
        
        # åˆå¹¶å±æ€§
        # å¯¹äºå¤šLinkæƒ…å†µï¼š
        # - kind_x: å–ç¬¬ä¸€ä¸ªï¼ˆå‡è®¾åŒä¸€ç»„çš„é“è·¯ç±»å‹ç›¸åŒï¼‰
        # - width: å–å¹³å‡
        # - length: æ±‚å’Œï¼ˆåŒå‘é“è·¯æ€»é•¿åº¦ï¼‰
        
        new_row = row.to_dict()
        
        if len(attrs_list) == 1:
            # å•Linkï¼Œç›´æ¥ä½¿ç”¨
            attr = attrs_list[0]
            new_row['kind_x'] = attr.get('kind_x', '')
            new_row['width'] = attr.get('width', np.nan)
            new_row['length'] = attr.get('length', np.nan)
            new_row['link_count'] = 1
            new_row['matched_links'] = attr['link_id']
        else:
            # å¤šLinkï¼Œéœ€è¦èšåˆ
            new_row['kind_x'] = attrs_list[0].get('kind_x', '')  # å–ç¬¬ä¸€ä¸ª

            new_row['width'] = attrs_list[0].get('width', '')
            
            lengths = [a.get('length') for a in attrs_list if pd.notna(a.get('length'))]
            new_row['length'] = sum(lengths) if lengths else np.nan
            
            new_row['link_count'] = len(attrs_list)
            new_row['matched_links'] = '+'.join([a['link_id'] for a in attrs_list])
        
        results.append(new_row)
    
    # è½¬æ¢ä¸ºDataFrame
    result_df = pd.DataFrame(results)
    
    # ç»Ÿè®¡
    print("\n" + "-" * 50)
    print("åŒ¹é…ç»“æœç»Ÿè®¡")
    print("-" * 50)
    
    original_ckpts = ckpt_df['å¡å£ç¼–å·'].nunique()
    matched_ckpts = result_df['å¡å£ç¼–å·'].nunique() if not result_df.empty else 0
    dropped_ckpts = len(set(ckpt_df['å¡å£ç¼–å·'].unique()) - set(result_df['å¡å£ç¼–å·'].unique() if not result_df.empty else []))
    
    print(f"åŸå§‹å¡å£æ•°: {original_ckpts}")
    print(f"åŒ¹é…æˆåŠŸ: {matched_ckpts}")
    print(f"åŒ¹é…å¤±è´¥: {dropped_ckpts}")
    
    # Linkæ•°é‡åˆ†å¸ƒ
    if not result_df.empty:
        link_count_dist = result_df.groupby('å¡å£ç¼–å·')['link_count'].first().value_counts().sort_index()
        print("\nLinkæ•°é‡åˆ†å¸ƒ:")
        for count, num in link_count_dist.items():
            print(f"  - {count} ä¸ªLink: {num} ä¸ªå¡å£")
    
    # é“è·¯ç±»å‹åˆ†å¸ƒ
    if not result_df.empty and 'kind_x' in result_df.columns:
        print("\né“è·¯ç±»å‹åˆ†å¸ƒ:")
        kind_mapping = CONFIG['road_kind_mapping']
        kind_stats = result_df.groupby('å¡å£ç¼–å·')['kind_x'].first().value_counts()
        total = kind_stats.sum()
        
        for kind_code, count in kind_stats.items():
            kind_name = kind_mapping.get(kind_code, 'æœªçŸ¥')
            pct = count / total * 100
            print(f"  - {kind_code} ({kind_name}): {count} ({pct:.1f}%)")
        
        # ä¿å­˜é“è·¯ç±»å‹ç»Ÿè®¡
        stats_df = pd.DataFrame({
            'ç±»å‹ä»£ç ': kind_stats.index,
            'å¡å£æ•°é‡': kind_stats.values
        })
        stats_df['é“è·¯ç±»å‹'] = stats_df['ç±»å‹ä»£ç '].map(kind_mapping).fillna('æœªçŸ¥')
        stats_df['å æ¯”'] = (stats_df['å¡å£æ•°é‡'] / total * 100).round(1).astype(str) + '%'
        
        stats_path = pm.get_processed_path(CONFIG['output_road_stats'])
        stats_df.to_csv(stats_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“Š é“è·¯ç±»å‹ç»Ÿè®¡: {stats_path.name}")
    
    # ä¿å­˜è¢«å‰”é™¤çš„è®°å½•
    if dropped_records:
        dropped_df = pd.DataFrame(dropped_records).drop_duplicates(subset=['å¡å£ç¼–å·'])
        dropped_path = pm.get_processed_path(CONFIG['output_dropped'])
        dropped_df.to_csv(dropped_path, index=False, encoding='utf-8-sig')
        print(f"âš ï¸ å‰”é™¤è®°å½•: {dropped_path.name} ({len(dropped_df)} ä¸ªå¡å£)")
    
    # ä¿å­˜ç»“æœ
    output_path = pm.get_processed_path(CONFIG['output_with_attrs'])
    result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print("\n" + "=" * 50)
    print("Stage 5 å®Œæˆ")
    print("=" * 50)
    print(f"è¾“å‡ºè®°å½•æ•°: {len(result_df)}")
    print(f"è¾“å‡ºå¡å£æ•°: {result_df['å¡å£ç¼–å·'].nunique() if not result_df.empty else 0}")
    print(f"âœ… è¾“å‡ºæ–‡ä»¶: {output_path.name}")
    
    return result_df


def run():
    """æ‰§è¡Œæµç¨‹"""
    return merge_road_attrs()


if __name__ == "__main__":
    run()
