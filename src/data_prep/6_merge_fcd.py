"""
Stage 6: èåˆæµ®åŠ¨è½¦æ•°æ®
å°†æµ®åŠ¨è½¦æ•°æ®ï¼ˆæµé‡ã€é€Ÿåº¦ã€çŠ¶æ€ï¼‰èåˆåˆ°å¡å£æ•°æ®

ç‰¹æ®Šå¤„ç†:
- å¤šLink (å¦‚ "100986499+60322527") éœ€è¦èšåˆæ‰€æœ‰Linkçš„æµ®åŠ¨è½¦æ•°æ®
- å‰”é™¤å…¨æ—¶æ®µæ— æµ®åŠ¨è½¦æ•°æ®çš„å¡å£
- ç»Ÿè®¡æ¯ä¸ªå¡å£çš„æµ®åŠ¨è½¦è¦†ç›–ç‡

ç”¨æ³•:
    python 6_merge_fcd.py
    python 6_merge_fcd.py --fcd-root "F:/jinan_temp"  # æŒ‡å®šæµ®åŠ¨è½¦æ•°æ®ç›®å½•
"""

import pandas as pd
import numpy as np
import os
import glob
import argparse
import sys
from pathlib import Path
from tqdm import tqdm

# === è·¯å¾„è®¾ç½® ===
current_file = Path(__file__).resolve()
src_dir = current_file.parent.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from utils.path_manager import pm


# === é…ç½®å‚æ•° ===
CONFIG = {
    # è¾“å…¥
    'input_checkpoint': 'checkpoint_with_road_attrs.csv',  # Stage 5 è¾“å‡º
    'fcd_root': r'F:\jinan_temp',  # æµ®åŠ¨è½¦æ•°æ®æ ¹ç›®å½•
    # é»‘åå•æ—¥æœŸ
    'skip_dates': ['20231122'],
    # æµ®åŠ¨è½¦æ•°æ®é…ç½®
    'fcd_cols': ['linkId', 'roadLen', 'dataTime', 'travelTime', 'status', 'carNum'],
    'invalid_roadlen': {0, 65535},
    'min_speed_kmh': 1,
    'max_speed_kmh': 120,
    # è¦†ç›–ç‡é˜ˆå€¼ (ä½äºæ­¤å€¼çš„å¡å£å°†è¢«å‰”é™¤)
    'min_coverage_rate': 0.75,
    # è¾“å‡º
    'output_final': 'final_training_data.csv',
    'output_dropped': 'report_dropped_no_fcd.csv',
    'output_coverage': 'report_fcd_coverage.csv',
}


def normalize_id(series):
    """æ ‡å‡†åŒ–IDå­—æ®µ"""
    return (series.astype(str)
            .str.replace(r'\.0$', '', regex=True)
            .str.strip())


def parse_link_ids(link_id_str):
    """
    è§£æ Link_ID å­—ç¬¦ä¸²ï¼Œå¯èƒ½æ˜¯å•ä¸ªæˆ–å¤šä¸ªï¼ˆç”¨+åˆ†éš”ï¼‰
    è¿”å›: list of link_ids
    """
    link_id_str = str(link_id_str).strip()
    if '+' in link_id_str:
        return [lid.strip() for lid in link_id_str.split('+')]
    else:
        return [link_id_str]


def load_fcd_for_dates(fcd_root, target_dates, target_links, skip_dates):
    """
    åŠ è½½æŒ‡å®šæ—¥æœŸçš„æµ®åŠ¨è½¦æ•°æ®
    
    Args:
        fcd_root: æµ®åŠ¨è½¦æ•°æ®æ ¹ç›®å½•
        target_dates: ç›®æ ‡æ—¥æœŸåˆ—è¡¨ (YYYYMMDD)
        target_links: ç›®æ ‡Link IDé›†åˆ
        skip_dates: è·³è¿‡çš„æ—¥æœŸåˆ—è¡¨
    
    Returns:
        èšåˆåçš„æµ®åŠ¨è½¦æ•°æ® DataFrame
    """
    fcd_cols = CONFIG['fcd_cols']
    fcd_dtypes = {
        'linkId': str, 'roadLen': 'float32', 'dataTime': 'float64',
        'travelTime': 'float32', 'status': 'float32', 'carNum': 'float32'
    }
    invalid_roadlen = CONFIG['invalid_roadlen']
    min_speed = CONFIG['min_speed_kmh']
    max_speed = CONFIG['max_speed_kmh']
    
    # è¿‡æ»¤é»‘åå•æ—¥æœŸ
    filtered_dates = [d for d in target_dates if d not in skip_dates]
    print(f"ç›®æ ‡æ—¥æœŸ: {len(target_dates)} å¤©")
    print(f"å‰”é™¤é»‘åå•å: {len(filtered_dates)} å¤©")
    if skip_dates:
        print(f"å·²è·³è¿‡æ—¥æœŸ: {skip_dates}")
    
    all_fcd_aggs = []
    
    for date_str in filtered_dates:
        date_dir = os.path.join(fcd_root, date_str)
        if not os.path.exists(date_dir):
            print(f"  âš ï¸ ç›®å½•ä¸å­˜åœ¨: {date_dir}")
            continue
        
        csv_files = glob.glob(os.path.join(date_dir, "*.csv"))
        if not csv_files:
            print(f"  âš ï¸ æ— CSVæ–‡ä»¶: {date_dir}")
            continue
        
        day_records = []
        
        for f_path in tqdm(csv_files, desc=f"è¯»å– {date_str}", unit="file", leave=False):
            try:
                df = pd.read_csv(f_path, usecols=fcd_cols, dtype=fcd_dtypes)
                if df.empty:
                    continue
                
                # ç­›é€‰ç›®æ ‡è·¯æ®µ
                df = df[df['linkId'].isin(target_links)]
                if df.empty:
                    continue
                
                # æ¸…æ´—å¼‚å¸¸æ•°æ®
                df = df[~df['roadLen'].isin(invalid_roadlen)]
                df = df[df['travelTime'] > 0]
                
                # é€Ÿåº¦è¿‡æ»¤
                speed_kmh = (df['roadLen'] / df['travelTime']) * 3.6
                df = df[(speed_kmh >= min_speed) & (speed_kmh <= max_speed)].copy()
                df['speed_kmh'] = speed_kmh.loc[df.index]
                
                if df.empty:
                    continue
                
                # æ—¶åŒºå¯¹é½ (+8h)
                dt_series = pd.to_datetime(df['dataTime'], unit='s') + pd.Timedelta(hours=8)
                df['hour_start'] = dt_series.dt.floor('h')
                
                day_records.append(df[['linkId', 'hour_start', 'carNum', 'speed_kmh', 'status']])
            
            except Exception as e:
                # é™é»˜å¤„ç†é”™è¯¯ï¼Œä¸æ‰“æ–­è¿›åº¦æ¡
                continue
        
        if day_records:
            day_df = pd.concat(day_records)
            # æŒ‰ linkId + hour_start èšåˆ
            agg_df = day_df.groupby(['linkId', 'hour_start']).agg({
                'carNum': 'sum',
                'speed_kmh': 'mean',
                'status': 'mean',
                'linkId': 'count'
            }).rename(columns={
                'carNum': 'fcd_flow',
                'speed_kmh': 'fcd_speed',
                'status': 'fcd_status',
                'linkId': 'fcd_record_count'
            }).reset_index()
            all_fcd_aggs.append(agg_df)
            print(f"  âœ“ {date_str}: {len(agg_df)} æ¡èšåˆè®°å½•")
    
    if not all_fcd_aggs:
        return pd.DataFrame()
    
    return pd.concat(all_fcd_aggs, ignore_index=True)


def merge_fcd(fcd_root=None):
    """
    èåˆæµ®åŠ¨è½¦æ•°æ®
    """
    print("=" * 60)
    print("Stage 6: èåˆæµ®åŠ¨è½¦æ•°æ®")
    print("=" * 60)
    
    if fcd_root:
        CONFIG['fcd_root'] = fcd_root
    
    # è¯»å–å¡å£æ•°æ®
    ckpt_path = pm.get_processed_path(CONFIG['input_checkpoint'])
    print(f"è¯»å–å¡å£æ•°æ®: {ckpt_path.name}")
    ckpt_df = pd.read_csv(ckpt_path)
    
    print(f"å¡å£æ•°: {ckpt_df['å¡å£ç¼–å·'].nunique()}")
    print(f"è®°å½•æ•°: {len(ckpt_df)}")
    
    # è§£ææ—¶é—´
    ckpt_df['start_dt'] = pd.to_datetime(ckpt_df['å¼€å§‹æ—¶é—´'])
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦çš„ Link IDï¼ˆå±•å¼€å¤šLinkï¼‰
    all_link_ids = set()
    link_id_mapping = {}  # {åŸå§‹Link_ID: [å±•å¼€åçš„link_ids]}
    
    for link_id_raw in ckpt_df['Link_ID'].unique():
        link_ids = parse_link_ids(link_id_raw)
        link_id_mapping[str(link_id_raw)] = link_ids
        all_link_ids.update(link_ids)
    
    print(f"å”¯ä¸€Linkæ•° (å±•å¼€å): {len(all_link_ids)}")
    
    # è·å–æ—¥æœŸèŒƒå›´
    target_dates = ckpt_df['start_dt'].dt.strftime('%Y%m%d').unique().tolist()
    
    # åŠ è½½æµ®åŠ¨è½¦æ•°æ®
    print(f"\næµ®åŠ¨è½¦æ•°æ®ç›®å½•: {CONFIG['fcd_root']}")
    fcd_df = load_fcd_for_dates(
        CONFIG['fcd_root'],
        target_dates,
        all_link_ids,
        CONFIG['skip_dates']
    )
    
    if fcd_df.empty:
        print("âŒ æœªåŠ è½½åˆ°ä»»ä½•æµ®åŠ¨è½¦æ•°æ®ï¼")
        return None
    
    print(f"\næµ®åŠ¨è½¦æ•°æ®æ€»é‡: {len(fcd_df)} æ¡")
    
    # åŒæ­¥å‰”é™¤é»‘åå•æ—¥æœŸçš„å¡å£æ•°æ®
    skip_dates = CONFIG['skip_dates']
    if skip_dates:
        skip_dt_list = pd.to_datetime(skip_dates, format='%Y%m%d')
        mask_keep = ~ckpt_df['start_dt'].dt.normalize().isin(skip_dt_list)
        removed_count = len(ckpt_df) - mask_keep.sum()
        ckpt_df = ckpt_df[mask_keep].copy()
        print(f"åŒæ­¥å‰”é™¤é»‘åå•æ—¥æœŸçš„å¡å£è®°å½•: {removed_count} æ¡")
    
    # === æ ¸å¿ƒï¼šä¸ºæ¯æ¡å¡å£è®°å½•åŒ¹é…æµ®åŠ¨è½¦æ•°æ® ===
    print("\næ­£åœ¨åŒ¹é…æµ®åŠ¨è½¦æ•°æ®...")
    
    # å°†æµ®åŠ¨è½¦æ•°æ®è½¬ä¸ºå­—å…¸ä¾¿äºå¿«é€ŸæŸ¥æ‰¾
    # key: (linkId, hour_start)
    fcd_df['hour_start'] = pd.to_datetime(fcd_df['hour_start'])
    fcd_dict = {}
    for _, row in fcd_df.iterrows():
        key = (row['linkId'], row['hour_start'])
        fcd_dict[key] = {
            'fcd_flow': row['fcd_flow'],
            'fcd_speed': row['fcd_speed'],
            'fcd_status': row['fcd_status'],
            'fcd_record_count': row['fcd_record_count']
        }
    
    results = []
    
    for idx, row in tqdm(ckpt_df.iterrows(), total=len(ckpt_df), desc="åŒ¹é…ä¸­"):
        link_id_raw = str(row['Link_ID'])
        hour_start = row['start_dt'].floor('h')
        
        # è·å–è¯¥å¡å£å¯¹åº”çš„æ‰€æœ‰Link
        link_ids = link_id_mapping.get(link_id_raw, [link_id_raw])
        
        # æ”¶é›†æ‰€æœ‰Linkçš„æµ®åŠ¨è½¦æ•°æ®
        fcd_values = []
        for lid in link_ids:
            key = (lid, hour_start)
            if key in fcd_dict:
                fcd_values.append(fcd_dict[key])
        
        new_row = row.to_dict()
        
        if fcd_values:
            # èšåˆå¤šä¸ªLinkçš„æµ®åŠ¨è½¦æ•°æ®
            new_row['fcd_flow'] = sum(v['fcd_flow'] for v in fcd_values)
            new_row['fcd_speed'] = np.mean([v['fcd_speed'] for v in fcd_values])
            new_row['fcd_status'] = np.mean([v['fcd_status'] for v in fcd_values])
            new_row['fcd_record_count'] = sum(v['fcd_record_count'] for v in fcd_values)
            new_row['fcd_matched'] = 1
        else:
            # æ— åŒ¹é…
            new_row['fcd_flow'] = 0
            new_row['fcd_speed'] = np.nan
            new_row['fcd_status'] = np.nan
            new_row['fcd_record_count'] = 0
            new_row['fcd_matched'] = 0
        
        results.append(new_row)
    
    result_df = pd.DataFrame(results)
    
    # è®¡ç®—æ¸—é€ç‡
    result_df['penetration_rate'] = result_df.apply(
        lambda r: r['fcd_flow'] / r['flow_std'] if r['flow_std'] > 0 else 0,
        axis=1
    )
    
    # === ç»Ÿè®¡è¦†ç›–ç‡å¹¶å‰”é™¤æ— æ•ˆå¡å£ ===
    print("\nè®¡ç®—æµ®åŠ¨è½¦è¦†ç›–ç‡...")
    
    coverage_stats = result_df.groupby('å¡å£ç¼–å·').agg(
        total_hours=('fcd_matched', 'count'),
        matched_hours=('fcd_matched', 'sum'),
        å¡å£åç§°=('å¡å£åç§°', 'first'),
        Link_ID=('Link_ID', 'first')
    ).reset_index()
    
    coverage_stats['coverage_rate'] = coverage_stats['matched_hours'] / coverage_stats['total_hours']
    
    # è¦†ç›–ç‡é˜ˆå€¼
    min_coverage = CONFIG['min_coverage_rate']
    
    # æ‰¾å‡ºä¸æ»¡è¶³è¦†ç›–ç‡è¦æ±‚çš„å¡å£
    low_coverage_ckpts = coverage_stats[coverage_stats['coverage_rate'] < min_coverage]['å¡å£ç¼–å·'].tolist()
    valid_ckpts = coverage_stats[coverage_stats['coverage_rate'] >= min_coverage]['å¡å£ç¼–å·'].tolist()
    
    # ç»†åˆ†ï¼šå®Œå…¨æ— æ•°æ® vs è¦†ç›–ç‡ä¸è¶³
    no_fcd_ckpts = coverage_stats[coverage_stats['matched_hours'] == 0]['å¡å£ç¼–å·'].tolist()
    low_but_has_fcd = [c for c in low_coverage_ckpts if c not in no_fcd_ckpts]
    
    print(f"\nå¡å£è¦†ç›–æƒ…å†µ (é˜ˆå€¼: {min_coverage:.0%}):")
    print(f"  - æ»¡è¶³è¦æ±‚ (â‰¥{min_coverage:.0%}): {len(valid_ckpts)} ä¸ª")
    print(f"  - å®Œå…¨æ— æ•°æ®: {len(no_fcd_ckpts)} ä¸ª (å‰”é™¤)")
    print(f"  - è¦†ç›–ç‡ä¸è¶³: {len(low_but_has_fcd)} ä¸ª (å‰”é™¤)")
    print(f"  - å‰”é™¤æ€»è®¡: {len(low_coverage_ckpts)} ä¸ª")
    
    # å‰”é™¤ä¸æ»¡è¶³è¦†ç›–ç‡è¦æ±‚çš„å¡å£
    final_df = result_df[result_df['å¡å£ç¼–å·'].isin(valid_ckpts)].copy()
    
    # æ¸…ç†ä¸´æ—¶åˆ—
    final_df.drop(columns=['start_dt', 'fcd_matched'], inplace=True, errors='ignore')
    
    # === è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š ===
    
    # 1. è¢«å‰”é™¤çš„å¡å£
    if low_coverage_ckpts:
        dropped_df = coverage_stats[coverage_stats['coverage_rate'] < min_coverage][
            ['å¡å£ç¼–å·', 'å¡å£åç§°', 'Link_ID', 'total_hours', 'matched_hours', 'coverage_rate']
        ].copy()
        dropped_df['åŸå› '] = dropped_df.apply(
            lambda r: 'å®Œå…¨æ— æµ®åŠ¨è½¦æ•°æ®' if r['matched_hours'] == 0 else f"è¦†ç›–ç‡ä¸è¶³({r['coverage_rate']:.1%}<{min_coverage:.0%})",
            axis=1
        )
        
        dropped_path = pm.get_processed_path(CONFIG['output_dropped'])
        dropped_df.to_csv(dropped_path, index=False, encoding='utf-8-sig')
        print(f"\nâš ï¸ å‰”é™¤å¡å£æŠ¥å‘Š: {dropped_path.name}")
    
    # 2. è¦†ç›–ç‡æŠ¥å‘Š (åªåŒ…å«ä¿ç•™çš„å¡å£)
    valid_coverage = coverage_stats[coverage_stats['coverage_rate'] >= min_coverage].copy()
    valid_coverage = valid_coverage.sort_values('coverage_rate', ascending=True)
    
    coverage_path = pm.get_processed_path(CONFIG['output_coverage'])
    valid_coverage.to_csv(coverage_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ“Š è¦†ç›–ç‡æŠ¥å‘Š: {coverage_path.name}")
    
    # è¦†ç›–ç‡ç»Ÿè®¡
    print("\nè¦†ç›–ç‡åˆ†å¸ƒ:")
    bins = [0, 0.25, 0.5, 0.75, 0.9, 1.0]
    labels = ['0-25%', '25-50%', '50-75%', '75-90%', '90-100%']
    valid_coverage['coverage_bin'] = pd.cut(valid_coverage['coverage_rate'], bins=bins, labels=labels)
    bin_counts = valid_coverage['coverage_bin'].value_counts().sort_index()
    for label, count in bin_counts.items():
        print(f"  - {label}: {count} ä¸ªå¡å£")
    
    avg_coverage = valid_coverage['coverage_rate'].mean()
    print(f"\nå¹³å‡è¦†ç›–ç‡: {avg_coverage:.1%}")
    
    # 3. ä¿å­˜æœ€ç»ˆæ•°æ®
    output_path = pm.get_processed_path(CONFIG['output_final'])
    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print("\n" + "=" * 60)
    print("Stage 6 å®Œæˆ")
    print("=" * 60)
    print(f"æœ€ç»ˆå¡å£æ•°: {final_df['å¡å£ç¼–å·'].nunique()}")
    print(f"æœ€ç»ˆè®°å½•æ•°: {len(final_df)}")
    print(f"âœ… è¾“å‡ºæ–‡ä»¶: {output_path.name}")
    
    return final_df


def run(fcd_root=None):
    """æ‰§è¡Œæµç¨‹"""
    return merge_fcd(fcd_root)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Stage 6: èåˆæµ®åŠ¨è½¦æ•°æ®")
    parser.add_argument('--fcd-root', type=str, default=None,
                        help='æµ®åŠ¨è½¦æ•°æ®æ ¹ç›®å½•')
    args = parser.parse_args()
    
    run(fcd_root=args.fcd_root)
